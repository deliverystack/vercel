<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="dns-prefetch" href="https://jpw3.com">
    <link rel="preconnect" href="https://jpw3.com">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Terminology and Concepts Cheat Sheet</title>
    <link rel="stylesheet" href="/styles.css"> 
    <link rel="canonical" href="https://jpw3.com/articles/2025/December/ai-cheatsheet.html">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FJ84G1V290"></script>
    <script>

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-FJ84G1V290');

  </script>
</head>
<body>

    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-XXXXXXX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <div id="page-layout"> 
        <header>
            <h1>AI Terminology and Concepts Cheat Sheet</h1>
            <nav class="breadcrumb"><a href="/">jpw3.com</a> › <a href="/articles/">Articles</a> › <a href="/articles/2025/">2025</a> › <a href="/articles/2025/December/">December</a> › <span class="breadcrumb-current">AI Cheat Sheet</span></nav>
        </header>

        <nav data-nosnippet>
            <ul><li class="current-branch"><details open><summary><a href="../../index.html">Articles</a></summary><ul><li><details><summary><a href="../../2013/index.html">2013</a></summary></details></li><li><details><summary><a href="../../2020/index.html">2020</a></summary></details></li><li><details><summary><a href="../../2021/index.html">2021</a></summary></details></li><li><details><summary><a href="../../2024/index.html">2024</a></summary></details></li><li class="current-branch"><details open><summary><a href="../index.html">2025</a></summary><ul><li class="current-branch"><details open><summary><a href="index.html">December</a></summary><ul><li class="current-file" title="articles/2025/December/ai-cheatsheet.md">AI Cheat Sheet</li><li><a class="nav-link" href="avoid-types.html" title="articles/2025/December/avoid-types.md">Avoid Types</a></li><li><a class="nav-link" href="bash-hist.html" title="articles/2025/December/bash-hist.md">BASH History</a></li><li><a class="nav-link" href="xclip.html" title="articles/2025/December/xclip.md">Capture to Clipboard</a></li><li><a class="nav-link" href="catholocism-origins.html" title="articles/2025/December/catholocism-origins.md">Catholicism Origin</a></li><li><a class="nav-link" href="static-site.html" title="articles/2025/December/static-site.md">Cheapest CMS</a></li><li><a class="nav-link" href="consumption-recommendations.html" title="articles/2025/December/consumption-recommendations.md">Consumption</a></li><li><a class="nav-link" href="my-ego.html" title="articles/2025/December/my-ego.md">Ego Struggle</a></li><li><a class="nav-link" href="evil-institutions.html" title="articles/2025/December/evil-institutions.md">Evil Is Institutional</a></li><li><a class="nav-link" href="fedora-shortcuts.html" title="articles/2025/December/fedora-shortcuts.md">Fedora Shortcuts</a></li><li><a class="nav-link" href="web-server.html" title="articles/2025/December/web-server.md">Fedora Web Server Startup</a></li><li><a class="nav-link" href="format-linkedin.html" title="articles/2025/December/format-linkedin.md">Format LinkedIn Content</a></li><li><a class="nav-link" href="gurdjieff-souls.html" title="articles/2025/December/gurdjieff-souls.md">Gurdjieff on Souls</a></li><li><a class="nav-link" href="markdown-cheatsheet.html" title="articles/2025/December/markdown-cheatsheet.md">Markdown Cheat Sheet</a></li><li><a class="nav-link" href="worst-mistakes.html" title="articles/2025/December/worst-mistakes.md">My Worst Mistakes</a></li><li><a class="nav-link" href="oscar-wilde.html" title="articles/2025/December/oscar-wilde.md">Oscar Wilde Quotes</a></li><li><a class="nav-link" href="relationship-person.html" title="articles/2025/December/relationship-person.md">Relationship/Person</a></li><li><a class="nav-link" href="fake-work.html" title="articles/2025/December/fake-work.md">Totalitarianism</a></li><li><a class="nav-link" href="tv-shows.html" title="articles/2025/December/tv-shows.md">TV Shows</a></li><li><a class="nav-link" href="us-border.html" title="articles/2025/December/us-border.md">US Border Prep</a></li><li><a class="nav-link" href="value-boredom.html" title="articles/2025/December/value-boredom.md">Value Boredom</a></li><li><a class="nav-link" href="vert-types.html" title="articles/2025/December/vert-types.md">Vert Types</a></li><li><a class="nav-link" href="wealth-power.html" title="articles/2025/December/wealth-power.md">Wealth and Power</a></li></ul></details></li><li><details><summary><a href="../November/index.html">November</a></summary></details></li><li><details><summary><a href="../October/index.html">October</a></summary></details></li><li><details><summary><a href="../May/index.html">May</a></summary></details></li><li><details><summary><a href="../February/index.html">February</a></summary></details></li></ul></details></li><li><details><summary><a href="../../psychology/index.html">Psychology</a></summary></details></li></ul></details></li><li><details ><summary><a href="../../../consciousness-stream/index.html">Mind Streams</a></summary><ul><li><details><summary><a href="../../../consciousness-stream/2025/index.html">2025</a></summary></details></li></ul></details></li><li><details ><summary><a href="../../../wip/index.html">WIP</a></summary><ul><li><a class="nav-link" href="../../../wip/ai-ldc.html" title="wip/ai-ldc.md">AI and LDCs</a></li><li><a class="nav-link" href="../../../wip/avoid-people.html" title="wip/avoid-people.md">Avoid People Who...</a></li><li><a class="nav-link" href="../../../wip/be-virtuous.html" title="wip/be-virtuous.md">Be Virtuous</a></li><li><a class="nav-link" href="../../../wip/dos-donts.html" title="wip/dos-donts.md">Do's and Don'ts</a></li><li><a class="nav-link" href="../../../wip/general-guidance.html" title="wip/general-guidance.md">General Guidance</a></li><li><a class="nav-link" href="../../../wip/green-flags.html" title="wip/green-flags.md">Green Flags</a></li><li><a class="nav-link" href="../../../wip/tech-actions.html" title="wip/tech-actions.md">Healthy Tech Actions</a></li><li><a class="nav-link" href="../../../wip/mixed.html" title="wip/mixed.md">mixed</a></li><li><a class="nav-link" href="../../../wip/us-christianity.html" title="wip/us-christianity.md">US Christianity</a></li><li><a class="nav-link" href="../../../wip/whats-wrong.html" title="wip/whats-wrong.md">USA, What's Wrong?</a></li><li><a class="nav-link" href="../../../wip/watchlist.html" title="wip/watchlist.md">watchlist</a></li></ul></details></li><li class="nav-separator"></li><li><a class="nav-link" href="../../../index.html" title="index.md">jpw3.com</a></li><li><a class="nav-link" href="../../../bibliography.html" title="bibliography.md">Bibliography</a></li><li><a class="nav-link" href="../../../command-lines.html" title="command-lines.md">Command Lines</a></li><li><a class="nav-link" href="../../../projects.html" title="projects.md">Github Projects</a></li><li><a class="nav-link" href="../../../topics.html" title="topics.md">Topics</a></li></ul>

            <form method="get" action="https://www.google.com/search">
              <input type="hidden" name="q" value="site:jpw3.com">
              &nbsp;&nbsp;<input type="text" name="q" placeholder="Search jpw3.com" style="width: 100px;">
              <input type="submit" value="Search">
            </form>
        </nav>
        
        <main>
            <p><img src="../../images/2025/December/aicheat.webp" alt="AI Cheat Sheet" /></p>
<h1>AI Terminology and Concepts Cheat Sheet</h1>
<p>This article presents a cheat sheet for terms and concepts related to Artificial Intelligence (AI). Much of this may assume LLM as a context. Some is copied directly from Wikipedia or google gemini output. Note that I have no education or expertise regarding AI; this effort definitely turned into a number of rabbit holes. Assume AI as the context for each of these terms and their explanations. These terms can be somewhat circular in use and hence appear in no particular order. If you have any terms or content to add or correct, please comment. I spent too much time on this already but would be happy to pass the markdown to anyone that might want to work on this in anyway.</p>
<p>Originally posted to:</p>
<ul>
<li>
<p><a href="https://deliverystack.net/2025/12/01/ai-terminlogy-and-concepts-cheat-sheet/" target="_blank" rel="noopener noreferrer">https://deliverystack.net/2025/12/01/ai-terminlogy-and-concepts-cheat-sheet/</a></p>
</li>
<li>
<p><strong>Artificial Intelligence (AI)</strong></p>
<ul>
<li>Computer program that simulates aspects appearing of intelligence</li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_intelligence" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Artificial_intelligence</a></li>
</ul>
</li>
<li>
<p><strong>Artificial General Intelligence (AGI)</strong></p>
<ul>
<li>Sometimes called human-level intelligence, AI that matches or exceeds human capabilities across almost all cognitive tasks.</li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Artificial_general_intelligence</a></li>
</ul>
</li>
<li>
<p><strong>Artificial Super Intelligence (ASI)</strong></p>
<ul>
<li>Surpasses the intelligence of most gifted human minds.</li>
<li>Significanltly exceeds human cognitive performance in virtually any domain</li>
<li><a href="https://en.wikipedia.org/wiki/Superintelligence" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Superintelligence</a></li>
</ul>
</li>
<li>
<p><strong>Chatbot</strong> </p>
<ul>
<li>System that can simulate textual or spoken conversations with users.</li>
<li><a href="https://en.wikipedia.org/wiki/Chatbot" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Chatbot</a></li>
</ul>
</li>
<li>
<p><strong>Large Language Model (LLM)</strong></p>
<ul>
<li>An LLM is just one type of system as an AI</li>
<li>Large: Trained on huge data set and many <em>Parameters</em>.</li>
<li>Language: Human language interface: input text (<em>Prompt</em>), output text. </li>
<li>Model: The trained <em>Neural Network</em> structure that performs the task (includes the <em>Parameters</em>). The model processes and outputs text in units called <em>Tokens</em>.</li>
<li>Predictive text generation: Repeatedly predicts the most probable next word in a sequence, given all the preceding words. </li>
<li><em>Neural Network</em> architectural basis, often Transformer architecture</li>
<li>Identifies patterns that represent grammar, semantics, context, and meaning</li>
<li>Typically <em>Generative</em>.</li>
<li>Can maintain <em>Context</em> over long sequences of interactions.</li>
<li>Applications include <em>Chatbots</em>, search engines, code generation, and customer service.</li>
<li><a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Large_language_model</a></li>
</ul>
</li>
<li>
<p><strong>Foundation Model</strong></p>
<ul>
<li>A large AI model trained on a broad range of data and can be adapted (fine-tuned) to a wide variety of downstream tasks.</li>
<li><a href="https://en.wikipedia.org/wiki/Foundation_model" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Foundation_model</a></li>
</ul>
</li>
<li>
<p><strong>Few-Shot Learning</strong></p>
<ul>
<li>AIs can use data within <em>Prompt</em>s to learn without changing the model's parameters. This is a form of in-context learning, not an extension of the core training data.</li>
<li><a href="https://en.wikipedia.org/wiki/Few-shot_learning" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Few-shot_learning</a></li>
</ul>
</li>
<li>
<p><strong>Prompt</strong></p>
<ul>
<li>Natural language text instructions used to direct Generative AI.</li>
</ul>
</li>
<li>
<p><strong>System Prompt, System Instructions</strong></p>
<ul>
<li>Invisible instructions given to the model by the developers or application builders to set its persona or behavior.</li>
</ul>
</li>
<li>
<p><strong>Prompt Engineering</strong></p>
<ul>
<li>Crafting <em>Prompts</em> to produce better output.</li>
<li><a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Prompt_engineering</a></li>
</ul>
</li>
<li>
<p><strong>Prompt Injection</strong></p>
<ul>
<li>An attack vector that uses innocuous-looking input to cause unintended behavior.</li>
<li><a href="https://en.wikipedia.org/wiki/Prompt_injection" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Prompt_injection</a></li>
</ul>
</li>
<li>
<p><strong>Attention Mechanism</strong></p>
<ul>
<li>Determines the importance of each component in a sequence relative to the other components in that sequence.</li>
<li>Computational cost grows quadratically with the length of the input sequence, which is the primary technical reason for the <em>Context Window</em> limit.</li>
<li><a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Attention_(machine_learning)</a></li>
</ul>
</li>
<li>
<p><strong>Context</strong></p>
<ul>
<li>Information provided to an AI in a single request to guide its response (AI's short-term memory).</li>
<li>Typically consists of the current <em>Prompt</em>, any conversation history, and any <em>System Prompt(s)</em>. </li>
</ul>
</li>
<li>
<p><strong>Context Window</strong></p>
<ul>
<li>Maximum size of an LLM's working memory, measured in <em>Tokens</em>.</li>
<li>Models discard (&quot;forget&quot;) older content when approaching the limit.</li>
<li><a href="https://en.wikipedia.org/wiki/Large_language_model#Attention_mechanism_and_context_window" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Large_language_model#Attention_mechanism_and_context_window</a></li>
</ul>
</li>
<li>
<p><strong>Generative AI</strong></p>
<ul>
<li>Produce &quot;novel&quot; output such as text, images, audiom video, or computer code, typically by reshaping training data</li>
<li><a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Generative_artificial_intelligence</a></li>
</ul>
</li>
<li>
<p><strong>Trainable Parameters (Weights and Biases)</strong></p>
<ul>
<li>Numerical values stored during training to encode the model's knowledge, patterns, and ability to generate language.</li>
<li>Automatically and iteratively adjusted during training to constitute a model's stored knowledge. A high weight means that input is very influential in the final output. Biases	allow neurons to activate even if all inputs are zero, giving the model a flexible baseline for making predictions.</li>
<li><a href="https://en.wikipedia.org/wiki/Weight_initialization" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Weight_initialization</a></li>
</ul>
</li>
<li>
<p><strong>Hyperparameters</strong></p>
<ul>
<li>External settings from model developers before or during the training process that control how the model learns and generates output.
<ul>
<li><strong>Architecture Hyperparameters</strong> define the model's structure, such as the number of layers in the network and the size of its internal dimensions.</li>
<li><strong>Training Hyperparameters</strong> control the learning process, such as the learning rate (how much the trainable parameters are adjusted in each step) and the batch size (how many samples the model processes before updating its parameters).</li>
<li><strong>Inference/Generation Hyperparameters</strong> control the model's output behavior when a user prompts it (after training is complete):</li>
<li><strong>Temperature</strong>: A higher temperature means more unpredictable, random, creative, and diverse responses.</li>
<li><strong>Max Tokens</strong>: Limits the length of the generated response.</li>
<li><strong>Top-p / Top-k</strong>: Sampling methods that refine the pool of possible next words from which to choose.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)</a></li>
</ul>
</li>
<li>
<p><strong>Neural Network (NN)</strong></p>
<ul>
<li>Computational model inspired by the structure and functions of biological neural networks such as brains and nervous systems.</li>
<li>Artificial neurons (nodes):
<ul>
<li>Loosely model the neurons in the brain connected by edges, which model synapses.</li>
<li>Receive, perform some calculation, and transmit signals (numbers) between connected neurons.</li>
<li>Are often aggregated into layers, where each performs a different transformation. </li>
</ul>
</li>
<li>Can learn from experience and derive conclusions from complex and seemingly unrelated sets of information.</li>
<li><a href="https://en.wikipedia.org/wiki/Neural_network_(machine_learning)" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Neural_network_(machine_learning)</a></li>
</ul>
</li>
<li>
<p><strong>Recurrent Neural Networks (RNN)</strong></p>
<ul>
<li>Designed to process sequential data, where the order of elements is important.</li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Recurrent_neural_network</a></li>
</ul>
</li>
<li>
<p><strong>Feedback Neural Network</strong></p>
<ul>
<li>Provide bottom-up and top-down design feedback to their input or previous layers, based on their outputs or subsequent layers. </li>
<li><a href="https://en.wikipedia.org/wiki/Feedback_neural_network" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Feedback_neural_network</a></li>
</ul>
</li>
<li>
<p><strong>Recursive Neural Network</strong></p>
<ul>
<li>Applies the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. </li>
<li><a href="https://en.wikipedia.org/wiki/Recursive_neural_network" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Recursive_neural_network</a></li>
</ul>
</li>
<li>
<p><strong>Transformer</strong></p>
<ul>
<li>As opposed to Recurrent Neural Networks (RNNs), Transformer Neural Networks simultaneously process multiple elements in an input sequence.</li>
<li>Input flow through a series of Encoder Blocks that refine the representation of the input sequence:
<ul>
<li><strong>Self-Attention Layer</strong>: Evaluates relative importance of elements in an input sequence</li>
<li><strong>Feed-Forward Layer</strong>: Provides non-linear transformations and introduces the model's complexity, allowing it to learn intricate patterns beyond simple attention calculations.</li>
<li><strong>Residual Connections and Layer Normalization</strong>: These connections allow gradients (error signals) to flow directly through the network during training, preventing them from fading away (the vanishing gradient problem) and speeding up convergence. Layer normalization stabilizes training.</li>
<li><strong>Encoder-Decoder</strong>: The classic Transformer architecture (used for translation) with separate blocks for processing input (Encoder) and generating output (Decoder). Modern LLMs like GPT are often Decoder-only models.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning)" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Transformer_(deep_learning)</a></li>
</ul>
</li>
<li>
<p><strong>Perceptron</strong></p>
<ul>
<li>Algorithm for supervised learning of binary classifiers, which can decide whether or not an input belongs to some specific class.</li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Perceptron</a></li>
</ul>
</li>
<li>
<p><strong>Generative Pre-trained Transformer (GPT)</strong></p>
<ul>
<li>LLM that uses Deep Learning Transformer architecture.</li>
<li>Pre-trained on large datasets of unlabeled content.</li>
<li><a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Generative_pre-trained_transformer</a></li>
</ul>
</li>
<li>
<p><strong>Training</strong></p>
<ul>
<li>Multi-stage process of iteratively adjusting the model's <em>Parameters</em> by exposing it to data.</li>
<li>Consists of pre-training, fine-tuning (instruction tuning), and alignment (<em>RLHF</em>).</li>
</ul>
</li>
<li>
<p><strong>Reinforcement Learning</strong></p>
<ul>
<li>Relates to controlling a reward signal to influence syste actions to align with goals.</li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Reinforcement_learning</a></li>
</ul>
</li>
<li>
<p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong></p>
<ul>
<li>Training a reward model to represent preferences used to train other models through reinforcement learning to align an intelligent agent with human preferences.</li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback</a></li>
</ul>
</li>
<li>
<p><strong>Retrieval Augmented Generation (RAG)</strong></p>
<ul>
<li>Enhances LLMs with the ability to incorporate data from a specific set of documents for domain-specific and/or updated information content not available in training data.</li>
<li><a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Retrieval-augmented_generation</a></li>
</ul>
</li>
<li>
<p><strong>Token</strong></p>
<ul>
<li>Numerical representation of a piece of text, a punctuation symbol, a formatting specifier, or a control character.</li>
<li>For the English language, a single token is typically approximately 0.75 words.</li>
<li><a href="https://en.wikipedia.org/wiki/Large_language_model#Tokenization" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Large_language_model#Tokenization</a></li>
</ul>
</li>
<li>
<p><strong>Reasoning Model, Reasoning Language Model (RLM)</strong></p>
<ul>
<li>LLM  trained to solve complex tasks that require multiple steps of logical reasoning.</li>
<li>Demonstrate superior performance on logic, mathematics, and programming tasks. </li>
<li>Ability to revisit and revise earlier reasoning steps and utilize additional computation during inference as a method to scale performance.</li>
<li><a href="https://en.wikipedia.org/wiki/Reasoning_model" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Reasoning_model</a></li>
</ul>
</li>
<li>
<p><strong>Model Context Protocol (MCP)</strong></p>
<ul>
<li>Framework for AI systems to share data and functionality</li>
<li><a href="https://en.wikipedia.org/wiki/Model_Context_Protocol" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Model_Context_Protocol</a></li>
</ul>
</li>
<li>
<p><strong>Machine Learning (ML)</strong></p>
<ul>
<li>Algorithms that apply existing data to generalise about unseen data or data as it appears to perform tasks in many fields without specific instructions.</li>
<li><a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Machine_learning</a></li>
</ul>
</li>
<li>
<p><strong>Deep Learning</strong> </p>
<ul>
<li>Uses multilayered neural networks to perform tasks such as classification, regression (estimating the relationship between a dependent variable and one or more independent variables), and representation learning (automatica discovery of representations needed for feature detection or classification from raw data). </li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Deep_learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_classification" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Statistical_classification</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Regression_analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_learning" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Feature_learning</a></li>
</ul>
</li>
<li>
<p><strong>Data Mining</strong></p>
<ul>
<li>This term is a misnomer; it means something more like sifting existing data than retrieving new data.</li>
<li>Identification of patterns in and  transformation of existing data.</li>
<li><a href="https://en.wikipedia.org/wiki/Data_mining" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Data_mining</a></li>
</ul>
</li>
<li>
<p><strong>Agent</strong></p>
<ul>
<li>Systems that perform tasks autonomously on behalf of users.</li>
<li>Optimally, through simulation of reasoning and planning with memory and context.</li>
</ul>
</li>
<li>
<p><strong>Traceability</strong></p>
<ul>
<li>The ability to follow the process from generated output back through the AI: 
<ul>
<li>What input training data it used.</li>
<li>The model and model version used.</li>
<li>The <em>Prompts</em> used.</li>
<li>Any <em>Agents</em> used.</li>
<li>Any additional steps including transformations.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Alignment</strong></p>
<ul>
<li>Attempts to ensure that a system stays directed towards specific goals, preferences, ethical principles, and other objectives without veering off course.</li>
<li><a href="https://en.wikipedia.org/wiki/AI_alignment" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/AI_alignment</a></li>
</ul>
</li>
<li>
<p><strong>Anthropomorphization</strong></p>
<ul>
<li>Considering an AI to have human-like characteristics or using terminology that specific to humans to describe machines.</li>
<li><a href="https://en.wikipedia.org/wiki/Anthropomorphism" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Anthropomorphism</a></li>
</ul>
</li>
<li>
<p><strong>Deification, Apotheosis</strong></p>
<ul>
<li>Users thinking of an AI as a deity or godlike entity. An AI can also suggest that its user is a deity or godlike entity.</li>
<li><a href="https://en.wikipedia.org/wiki/Apotheosis" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Apotheosis</a></li>
</ul>
</li>
<li>
<p><strong>AI Psychosis, Chatbot Psychosis</strong></p>
<ul>
<li>Development of worsening psychosis, such as paranoia and delusions, in connection with AI use.</li>
<li><a href="https://en.wikipedia.org/wiki/Chatbot_psychosis" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Chatbot_psychosis</a></li>
</ul>
</li>
<li>
<p><strong>Confabulation (often anthropomorphized as Hallucination)</strong></p>
<ul>
<li>AI output that contains false or misleading information presented as fact.</li>
<li><a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)</a></li>
</ul>
</li>
<li>
<p><strong>Generative Image Model</strong>, <strong>Text-to-Image Model</strong>, <strong>Diffusion Model</strong>, <strong>Large Vision Model</strong></p>
<ul>
<li>Image generation models that I didn't research. </li>
</ul>
</li>
<li>
<p><strong>Text-to-Video Model</strong>, <strong>Large Video Model</strong></p>
<ul>
<li>Video generation models that I didn't research.</li>
</ul>
</li>
<li>
<p><strong>Text-to-Audio Model</strong>, <strong>Text-to-Music Model</strong>, <strong>Audio Language Model</strong></p>
<ul>
<li>Audio generation models that I didn't research.</li>
</ul>
</li>
<li>
<p><strong>Multimodal Model</strong></p>
<ul>
<li>Single, large foundation models capable of handling all of data.</li>
<li><a href="https://en.wikipedia.org/wiki/Multimodal_learning" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Multimodal_learning</a></li>
</ul>
</li>
<li>
<p><strong>Inference</strong>: The process of using a trained machine learning model to make predictions or decisions on new, previously unseen data.</p>
</li>
<li>
<p><strong>Sycophantic</strong>: //TODO: Sycophantic</p>
</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="https://www.linkedin.com/pulse/new-language-ai-john-west-xpstc/" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/pulse/new-language-ai-john-west-xpstc/</a></li>
</ul>
<p>This might be the best place to comment:</p>
<ul>
<li><a href="https://www.linkedin.com/feed/update/urn%3Ali%3Ashare%3A7401457902517071872" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/feed/update/urn%3Ali%3Ashare%3A7401457902517071872</a></li>
</ul>

        </main>
        
        <footer>
            <p>Source: <code>/articles/2025/December/ai-cheatsheet.md</code> | Created : 2025-12-20 | Last modified: 2025-12-20</p>
        </footer>
    </div>
</body>
</html>